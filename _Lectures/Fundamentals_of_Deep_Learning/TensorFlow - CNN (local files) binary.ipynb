{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Access images located in a local directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the os library gives you  access to the file system\n",
    "import os\n",
    "\n",
    "# We will be using images of fruit (apples and oranges),\n",
    "# so our classification will be binary (apple or orange)\n",
    "\n",
    "\n",
    "# listdir returns a list of the files in the directory; len() returns the number of files/items in the list\n",
    "# display the number of images in the various folders\n",
    "\n",
    "# the directory location is relative to the location of the Notebook\n",
    "\n",
    "# Training Images\n",
    "# \"fruit/train/apple\"\n",
    "# \"fruit/train/orange\"\n",
    "\n",
    "# Test Images\n",
    "# \"fruit/test/apple\"\n",
    "# \"fruit/test/orange\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of train apple images:  75\n",
      "The number of train orange images:  72\n",
      "The number of test apple images:  19\n",
      "The number of test orange images:  18\n"
     ]
    }
   ],
   "source": [
    "# determine the number of images of each type\n",
    "\n",
    "# 147 training images\n",
    "print(\"The number of train apple images: \", len(os.listdir(\"fruit/train/apple\")))\n",
    "print(\"The number of train orange images: \", len(os.listdir(\"fruit/train/orange\")))\n",
    "\n",
    "#37 test images\n",
    "print(\"The number of test apple images: \", len(os.listdir(\"fruit/test/apple\")))\n",
    "print(\"The number of test orange images: \", len(os.listdir(\"fruit/test/orange\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\r560847\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "# larger input shapes tend to require more convolutions to get them down to size\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    # This is the first convolution\n",
    "    tf.keras.layers.Conv2D(16, (3,3), padding='same', activation='relu', input_shape=(150, 150, 3)),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    # The second convolution\n",
    "    tf.keras.layers.Conv2D(32, (3,3), padding='same', activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    # The third convolution\n",
    "    tf.keras.layers.Conv2D(64, (3,3), padding='same', activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    # The fourth convolution\n",
    "    tf.keras.layers.Conv2D(64, (3,3), padding='same', activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    # The fifth convolution\n",
    "    tf.keras.layers.Conv2D(64, (3,3), padding='same', activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    # Flatten the results to feed into a Dense (fully-connected) neural network\n",
    "    tf.keras.layers.Flatten(),\n",
    "    # 512 neuron hidden layer\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    # Only 1 output neuron. It will contain a value from 0-1; 0 is for 'apple' and 1 is for 'orange'\n",
    "    # a single (binary) output neuron requires a 'sigmoid' activation\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "\n",
    "# complie model with selected parameters; since the output is binary, 'binary_crossentropy' is used\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 150, 150, 16)      448       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 75, 75, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 75, 75, 32)        4640      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 37, 37, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 37, 37, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 18, 18, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 18, 18, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 9, 9, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 9, 9, 64)          36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 4, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 622,753\n",
      "Trainable params: 622,753\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# view the journey of the image through the convolutions\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize the data and generate labels for the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 147 images belonging to 2 classes.\n",
      "Found 37 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# All images will be normalized (rescaled by 1./255)\n",
    "train_data_generator = ImageDataGenerator(rescale=1/255)\n",
    "validate_data_generator = ImageDataGenerator(rescale=1/255)\n",
    "\n",
    "# Flow training images in batches of 30 using train_data_generator\n",
    "train_generator = train_data_generator.flow_from_directory(\n",
    "        'fruit/train',  # This is the source directory for training images \n",
    "                        # It contains sub-directories(whose names will be the labels) that contain images\n",
    "        target_size=(150, 150),  # All images will be resized to 150x150\n",
    "        batch_size=30,\n",
    "        # Since we use binary_crossentropy loss, we need binary labels\n",
    "        class_mode='binary')\n",
    "\n",
    "validate_generator = validate_data_generator.flow_from_directory(\n",
    "        'fruit/test',  # This is the source directory for training images \n",
    "                            # that contains sub-directories(whose names will be the labels) that contain images\n",
    "        target_size=(150, 150),  # All images will be resized to 150x1500\n",
    "        batch_size=10,\n",
    "        # Since we use binary_crossentropy loss, we need binary labels\n",
    "        class_mode='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the CNN (using the generated labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\r560847\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\r560847\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\PIL\\Image.py:952: UserWarning: Palette images with Transparency   expressed in bytes should be converted to RGBA images\n",
      "  ' expressed in bytes should be converted ' +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 1s 316ms/step - loss: 0.6562 - acc: 0.4865\n",
      "5/5 [==============================] - 5s 992ms/step - loss: 0.7078 - acc: 0.5986 - val_loss: 0.6562 - val_acc: 0.4865\n",
      "Epoch 2/5\n",
      "4/4 [==============================] - 1s 256ms/step - loss: 0.4407 - acc: 0.9459\n",
      "5/5 [==============================] - 4s 716ms/step - loss: 0.6109 - acc: 0.7823 - val_loss: 0.4407 - val_acc: 0.9459\n",
      "Epoch 3/5\n",
      "4/4 [==============================] - 1s 240ms/step - loss: 0.2145 - acc: 0.8919\n",
      "5/5 [==============================] - 3s 632ms/step - loss: 0.2671 - acc: 0.9524 - val_loss: 0.2145 - val_acc: 0.8919\n",
      "Epoch 4/5\n",
      "4/4 [==============================] - 1s 242ms/step - loss: 0.0121 - acc: 1.0000\n",
      "5/5 [==============================] - 3s 646ms/step - loss: 0.1321 - acc: 0.9592 - val_loss: 0.0121 - val_acc: 1.0000\n",
      "Epoch 5/5\n",
      "4/4 [==============================] - 1s 236ms/step - loss: 0.0137 - acc: 1.0000\n",
      "5/5 [==============================] - 3s 613ms/step - loss: 0.0800 - acc: 0.9592 - val_loss: 0.0137 - val_acc: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# fit the model using the generated labels\n",
    "\n",
    "history = model.fit_generator(\n",
    "      train_generator,\n",
    "      steps_per_epoch=5,  \n",
    "      epochs=5,\n",
    "    \n",
    "      # validate the trained model on unseen (test) images\n",
    "      validation_data = validate_generator,\n",
    "      validation_steps=4,\n",
    "      verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "\n",
    "Experiment with different models and see how the accuracy results differ. In particular change the following parameters:\n",
    "\n",
    "Set the input padding to 'valid' from 'same'.\n",
    "\n",
    "Modify the size that all of the images are initially sized to (this parameter will also need to be set to the same value in the generator).\n",
    "\n",
    "Add or remove convolutional/pooling layers. \n",
    "\n",
    "Experiment with the number of neurons used in the Dense (hidden) layer.\n",
    "\n",
    "Modify the number of epochs used in model.fit_generator()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "\n",
    "# a list of images (from a directory/folder) that will be used to validate the model's predictions\n",
    "validating_images = os.listdir(\"fruit/validating_images\")\n",
    "\n",
    "# predicting the class of unseen images\n",
    "for file_name in validating_images:\n",
    "    path = 'fruit/validating_images/' + file_name\n",
    "    img = image.load_img(path, target_size = (150,150))\n",
    "    x = image.img_to_array(img)\n",
    "    x = np.expand_dims(x, axis = 0)  # the model expects a batch, make a batch of 1\n",
    "    images = np.vstack([x])\n",
    "    \n",
    "    classes = model.predict(images, batch_size = 1)\n",
    "    print(classes[0])\n",
    "    if classes[0] >.05:\n",
    "        print(file_name + \" is an orange.\")\n",
    "    else:\n",
    "        print(file_name + \" is an apple.\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
